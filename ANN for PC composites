#Import necessary libraries
import numpy as np
import random
import pandas as pd
import pickle
from pickle  import load
from sklearn import preprocessing
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
import time
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from pickle import load
from tensorflow.keras import layers
from tensorflow.keras import regularizers

# Load data from a pickle file
data = pd.read_pickle('data.pkl')

# Select data for PC composites
PC = data.loc[data['Material']=='PC'].reset_index(drop=True)

# Random selection of temperatures for creating training and testing data
gb = PC.groupby(['Temperature']).groups  #group PC based on temperature values

# Split testing data
test_data = pd.DataFrame()
train_data = PC
for i in range (2): #select two random temperatures as test data  
    res = random.choice(list(gb.items()))
    ind_res = np.array(res[1])
    test_data = pd.concat([test_data, PC.iloc[ind_res]])

#Split training data
train_data = PC.drop(test_data.index).reset_index() #select the other as training data

# Reset indeces for training and testing data
train_data.reset_index()
test_data.reset_index()

# This script scales the training and testing data using a standard scaler for the main model
input_train = train_data.loc[:,['Filler', 'Temperature', 'Strain']] # Select input training data
scaler_input = preprocessing.StandardScaler() # Instantiate a StandardScaler object
X_train = scaler_input.fit_transform(input_train) # Fit the scaler to the input training data
output_train = train_data.loc[:,['Stress']] # Select output training data
scaler_output = preprocessing.StandardScaler() # Instantiate a StandardScaler object
Y_train = scaler_output.fit_transform(output_train) # Fit the scaler to the output training data
input_test = test_data.loc[:,['Filler', 'Temperature', 'Strain']] # Select input testing data
X_test = scaler_input.transform(input_test) # Fit the scaler to the input testing data
output_test = test_data.loc[:,['Stress']] # Select output testing data
Y_test = scaler_output.transform(output_test) # Fit the scaler to the output testing data

# This script builds an ANN model for main model
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10) # Define early stopping criteria
# Define layers
model = Sequential()
model.add(Dense(6000, input_shape=(3,), activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-6, l2=1e-5)))
model.add(Dropout(0.1))
model.add(Dense(6000, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-6, l2=1e-5)))
model.add(Dropout(0.1))
model.add(Dense(6000, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-6, l2=1e-5)))
model.add(Dropout(0.1))
model.add(Dense(6000, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-6, l2=1e-5)))
model.add(Dropout(0.1))
model.add(Dense(1))
opt = tf.keras.optimizers.Adam(learning_rate=0.00001) # Define Adam optimizer and learning rate
model.compile(optimizer=opt,
             loss='mean_squared_error',
             metrics=['MSE']) # Define loss function
history=model.fit(X_train, Y_train, batch_size=16, epochs=200, verbose=1, callbacks=callback) # Fit the main model to the training data

# Plot loss vs epochs for the main model
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

# This script scales the training and testing data using a standard scaler for the failure model
input_train_f = train_data.loc[:,['Filler', 'Temperature']] # Select the input training data
scaler_input_f = preprocessing.StandardScaler() # Instantiate a StandardScaler object
X_train_f = scaler_input_f.fit_transform(input_train_f) # Fit the scaler to the input training data
output_train_f = train_data.loc[:,['SatF']] # Select output training data
scaler_output_f = preprocessing.StandardScaler() # Instantiate a StandardScaler object
Y_train_f = scaler_output_f.fit_transform(output_train_f) # Fit the scaler to the output training data
input_test_f = test_data.loc[:,['Filler', 'Temperature']] # Select the input testing data
X_test_f = scaler_input_f.transform(input_test_f) # Fit the scaler to the input testing data
output_test_f = test_data.loc[:,['SatF']] # Select the output testing data
Y_test_f = scaler_output_f.transform(output_test_f) # Fir the scalet to the output testing data

# This script builds an ANN model for failure model
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10) # Define early stopping criteria
model_f = Sequential() # Define layers
model_f.add(Dense(2000, input_shape=(2,), activation='relu'))
model_f.add(Dropout(0.1))
model_f.add(Dense(2000, activation='relu'))
model_f.add(Dropout(0.1))
model_f.add(Dense(2000, activation='relu'))
model_f.add(Dropout(0.1))
model_f.add(Dense(2000, activation='relu'))
model_f.add(Dropout(0.1))
model_f.add(Dense(1))
opt = tf.keras.optimizers.Adam(learning_rate=0.0001) # Define Adam optimizer and learning rate
model_f.compile(optimizer=opt,
             loss='mean_squared_error',
             metrics=['MSE']) # Define loss function
history_f=model_f.fit(X_train_f, Y_train_f, batch_size=32, epochs=200, verbose=1, callbacks=callback) # Fit the main model to the training data

# Plot loss vs epochs for failure model
plt.plot(history_f.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

# Calculate and print model evaluation metrics
pred_train = model.predict(X_train) # Predict outputs for training data
pred_test = model.predict(X_test) # Predict outputs for testing data
pred_train_f = model_f.predict(X_train_f) # Predict outputs for failure training data
pred_test_f = model_f.predict(X_test_f) # Predict outputs for failure testing data
RMSE_train = np.sqrt(mean_squared_error(scaler_output.inverse_transform(Y_train.reshape(-1,1)), scaler_output.inverse_transform(pred_train.reshape(-1,1)))) # Calculate RMSE for training data
RMSE_test = np.sqrt(mean_squared_error(scaler_output.inverse_transform(Y_test.reshape(-1,1)), scaler_output.inverse_transform(pred_test.reshape(-1,1)))) # Calculate RMSE for testing data
RMSE_train_f = np.sqrt(mean_squared_error(Y_train_f, pred_train_f)) # Calculate RMSE for failure training data
RMSE_test_f = np.sqrt(mean_squared_error(Y_test_f, pred_test_f)) # Calculate RMSE for failure testing data
print("RMSE for training data= ",RMSE_train)
print("RMSE for testing data= ",RMSE_test)
print("R2 for training data = ", r2_score(Y_train, pred_train))
print("R2 for testing data = ", r2_score(Y_test, pred_test))
print("RMSE for training data (failure model)= ",RMSE_train_f)
print("RMSE for testing data (failure model)= ",RMSE_test_f)

# This script creates a dataframe by combining input and output data
input_data = PC.loc[:,['Filler', 'Temperature', 'Strain']] # Create input dataframe (entire data)
input_data_scaled = scaler_input.transform(input_data) # Create scaled input data (entire data)
stress_scaled = model.predict(input_data_scaled) # Predict outputs for input data in scaled format
stress_pred = pd.DataFrame(scaler_output.inverse_transform(stress_scaled)) # Create dataframe of predictions in original format
stress_pred.columns=['Stress Predicted'] # Name the prediction column for the final dataframe
df = pd.concat([PC.reset_index(drop=True), stress_pred], axis=1) # Create the final dataframe

# This script plots curves of stress vs strain data
fig = plt.figure(figsize=(10,14), dpi=200) # Define plots' properties
i=0
for f in [0, 10, 20, 30, 40]: # Filler amounts
    i=i+1
    plt.subplot(3,2,i) # Define the position of subplots
    for t in [-20, 23, 40, 60, 90, 120]: # Temperature amounts
        if not df.loc[(df['Filler'] == f) & (df['Temperature'] == t)].empty: # Check if there is missing value in the dataframe
            subset = df.loc[(df['Filler'] == f) & (df['Temperature'] == t)] # Create a subset of dataframe
            if (t==40 or t==90): # In case of testing data:
                plt.plot(subset.loc[:,'Strain'], subset.loc[:,'Stress Predicted'], 'r', linestyle='--') # Plot predicted stress vs strain
                plt.scatter(subset.loc[:,'Strain'], subset.loc[:,'Stress'],  marker = '*', edgecolors = 'r', facecolors='w',
                           s=10) # Plot original stress vs strain
            else: # In case of training data:
                plt.plot(subset.loc[:,'Strain'], subset.loc[:,'Stress Predicted'], 'k') # Plot predicted stress vs strain
                plt.scatter(subset.loc[:,'Strain'], subset.loc[:,'Stress'],  marker = '*', edgecolors = 'k', facecolors='w',
                           s=10) # plot original stress vs strain
            plt.title('PC; %s wt%%' % f, fontsize=12) # Print title of plots
            plt.xlabel('Strain (%)', fontsize=12) # Print label for x-axis
            plt.ylabel('Stress (MPa)', fontsize=12) # Print label for y-axis
plt.tight_layout() # Adjust the spacing between subplots
plt.show() # show the plot
